---
layout: post
title:  "[1] Learning Spatio-Temporal Transformer for Visual Tracking"
date:   2021-07-23
image: /images/1.png
categories: research
authors: "<strong>Bin Yan</strong>, Houwen Peng*, Jianlong Fu, Dong Wang*, Huchuan Lu"
venue: "ICCV"
arxiv: https://arxiv.org/pdf/2103.17154.pdf
code: https://github.com/researchmm/Stark#stark
---

We present a new tracking architecture with an encoder-decoder transformer as the key component. 
The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial 
positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem. The whole method is end-to-end, does not need any postprocessing steps, thus largely simplifying
existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on five challenging short-term and long-term benchmarks, while running at real-time speed, being 6x faster than Siam R-CNN.